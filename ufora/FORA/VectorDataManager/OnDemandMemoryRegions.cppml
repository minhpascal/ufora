/***************************************************************************
    Copyright 2016 Ufora Inc.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
 
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/


#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <signal.h>
#include <boost/lexical_cast.hpp>


#include "../../core/Logging.hpp"
#include "../../core/lassert.hpp"
#include "OnDemandMemoryRegions.hppml"

namespace {

//return an interator to the largest key that's <= 'inKey', or "end" if none
//exists
template<class K, class V>
typename std::map<K,V>::const_iterator block_containing(const std::map<K,V>& inMap, const K& inKey)
	{
	if (inMap.size() == 0)
		return inMap.end();

	auto it = inMap.lower_bound(inKey);

	if (it != inMap.end() && it->first == inKey)
		return it;

	if (it == inMap.begin())
		return inMap.end();

	it--;

	lassert(!(inKey < it->first));

	return it;
	}

boost::mutex singletonMutex;
std::set<OnDemandMemoryRegions*> activeMemoryRegions;
struct sigaction* oldSigActionPtr;

void cleanupAndTerminateProcess()
	{
	boost::mutex::scoped_lock lock(singletonMutex);

	LOG_CRITICAL << "OnDemandMemoryRegions terminating process.";
	fflush(stdout);
	fflush(stderr);

	for (auto regionPtr: activeMemoryRegions)
		regionPtr->preTerminationCleanup();

	_exit(1);
	}

void sighandler(int i, siginfo_t* info, void* dat)
	{
	OnDemandMemoryRegions* regionPtr = OnDemandMemoryRegions::curRegionPtr();

	if (regionPtr)
		{
		regionPtr->blockOnAddress(info->si_addr);
		return;
		}

	LOG_CRITICAL << "Segmentation fault without an active OnDemandMemoryRegions taking responsibility for it.";

	cleanupAndTerminateProcess();
	}

}

OnDemandMemoryRegions::OnDemandMemoryRegions(
            boost::function1<void, BlockingThread> inOnThreadBlocked,
            boost::function0<void*> inOnExtractCurrentBlockedThreadInfo,
            std::string sharedMemoryPrefix
            ) : 
		mSharedMemoryPrefix(sharedMemoryPrefix + "_0"),
		mOnThreadBlocked(inOnThreadBlocked),
		mOnExtractCurrentBlockedThreadInfo(inOnExtractCurrentBlockedThreadInfo),
		mTotalBytesShared(0),
		mTotalBytesMappable(0)
	{
		{
		boost::mutex::scoped_lock lock(singletonMutex);

		auto anyHaveOurPrefix = [&]() {
			for (auto other: activeMemoryRegions)
				if (other->mSharedMemoryPrefix == mSharedMemoryPrefix)
					return true;
			return false;
			};

		long count = 1;
		while (anyHaveOurPrefix())
			{
			count++;
			mSharedMemoryPrefix = sharedMemoryPrefix + "_" + boost::lexical_cast<std::string>(count);
			}

		activeMemoryRegions.insert(this);

		if (activeMemoryRegions.size() == 1)
			{
			oldSigActionPtr = new struct sigaction;

			struct sigaction new_action;

			memset(&new_action, 0, sizeof(new_action));

			new_action.sa_sigaction = &sighandler;
			new_action.sa_flags = SA_SIGINFO;
  			sigemptyset(&new_action.sa_mask);

			int code = sigaction(SIGSEGV, &new_action, oldSigActionPtr);

			if (code)
				{
				LOG_ERROR << "Failed to install signal handler: " << strerror(errno);
				fflush(stdout);
				fflush(stderr);
				_exit(0);
				}
			}
		}
	}

OnDemandMemoryRegions* OnDemandMemoryRegions::curRegionPtr()
	{
	boost::mutex::scoped_lock lock(singletonMutex);

	for (auto regionPtr: activeMemoryRegions)
		{
		void* threadInfo = regionPtr->currentThreadInfo();
		if (threadInfo)
			return regionPtr;
		}

	return nullptr;
	}

uint64_t OnDemandMemoryRegions::pageSize() const
	{
	return getpagesize();
	}

OnDemandMemoryRegions::~OnDemandMemoryRegions()
	{
	lassert_dump(!mBlockingThreads.size(), "makes no sense to have any threads blocking at teardown");

		{
		boost::mutex::scoped_lock lock(singletonMutex);

		activeMemoryRegions.erase(this);

		if (activeMemoryRegions.size() == 0)
			{
			int code = sigaction(SIGSEGV, oldSigActionPtr, nullptr);

			lassert_dump(code == 0, "sigaction failed: " << strerror(errno));

			delete oldSigActionPtr;
			}
		}

	cleanup_();
	}

void OnDemandMemoryRegions::preTerminationCleanup()
	{
	for (auto mappableRegion: mMappableMemoryRecords)
		if (::munmap(mappableRegion.second.address(), mappableRegion.second.size()))
			{
			LOG_CRITICAL << "failed to munmap: " << strerror(errno);
			fflush(stdout);
			fflush(stderr);
			}

	for (auto mappableRegion: mSharedMemoryRecords)
		{
		close(mappableRegion.second.fileDescriptor());		
		if (shm_unlink(mappableRegion.second.name().c_str()))
			{
			LOG_CRITICAL << "failed to unlink " << mappableRegion.second.name() << ": " << strerror(errno);
			fflush(stdout);
			fflush(stderr);
			}
		}	
	}

void* OnDemandMemoryRegions::currentThreadInfo()
	{
	return mOnExtractCurrentBlockedThreadInfo();
	}

void OnDemandMemoryRegions::cleanup_()
	{
	for (auto mappableRegion: mMappableMemoryRecords)
		if (::munmap(mappableRegion.second.address(), mappableRegion.second.size()))
			LOG_ERROR << "munmap failed: " << strerror(errno);
	for (auto mappableRegion: mSharedMemoryRecords)
		{
		if (::munmap(mappableRegion.second.address(), mappableRegion.second.size()))
			LOG_ERROR << "munmap failed: " << strerror(errno);

		close(mappableRegion.second.fileDescriptor());
		
		shm_unlink(mappableRegion.second.name().c_str());
		}
	}

void OnDemandMemoryRegions::blockOnAddress(void* addr)
	{
	boost::mutex::scoped_lock lock(mMutex);

	auto addrCharPtr = (uint8_t*)addr;

	//find the memory pointer that's closest
	auto it = block_containing(mMappableMemoryRecords, addrCharPtr);

	if (it == mMappableMemoryRecords.end() || (addrCharPtr - it->second.address()) >= it->second.size())
		{
		LOG_CRITICAL << "Address " << addr << " doesn't correspond to any address in the mapping pool.";
		cleanupAndTerminateProcess();
		}

	void* info = mOnExtractCurrentBlockedThreadInfo();

	if (mBlockingThreads.find(info) != mBlockingThreads.end())
		{	
		LOG_CRITICAL << "The same thread is marked as blocking twice! This should be impossible.";
		cleanupAndTerminateProcess();
		}

	do {
		//see if there is a mapped region for this already
		auto it_2 = block_containing(mMappedMemoryRecords, addrCharPtr);
		if (it_2 != mMappedMemoryRecords.end() && (addrCharPtr - it_2->second.address()) < it_2->second.bytesToMap())
			{
			mBlockingThreads.erase(info);
			return;
			}
		
		boost::shared_ptr<boost::condition> conditionPtr(new boost::condition);

		mBlockingThreads[info] = BlockingThread(it->second.address(), addrCharPtr - it->second.address(), info);

		mBlockingThreadConditions[addrCharPtr].push_back(conditionPtr);

		lock.unlock();
		mOnThreadBlocked(mBlockingThreads[info]);
		lock.lock();

		conditionPtr->wait(lock);
		
		} while (true);
	}

void OnDemandMemoryRegions::setSignalMaskOnCurrentThread()
	{
	sigset_t signal_mask;
	sigemptyset(&signal_mask);
	sigaddset(&signal_mask, SIGSEGV);
	lassert_dump(
		pthread_sigmask(SIG_UNBLOCK, &signal_mask, nullptr) == 0,
		"failed to set signal mask: " << strerror(errno)
		);
	}

uint8_t* OnDemandMemoryRegions::allocateSharedRegion(uint64_t bytecount)
	{
	boost::mutex::scoped_lock lock(mMutex);

	std::string segmentName = mSharedMemoryPrefix + "_" + boost::lexical_cast<std::string>(mSharedMemoryRecords.size());

	int shm_fd = shm_open(segmentName.c_str(), O_RDWR | O_CREAT, 0x777);

	if (shm_fd == -1)
		{
		LOG_ERROR << "Failed to allocate a shared-memory segment of size " << bytecount / 1024 / 1024.0 << " MB: " 
			<< strerror(errno);

		return nullptr;
		}

	if (ftruncate(shm_fd, bytecount) == -1)
		{
		LOG_ERROR << "Failed to allocate a shared-memory segment of size " << bytecount / 1024 / 1024.0 << " MB: " 
			<< strerror(errno);

		shm_unlink(segmentName.c_str());

		return nullptr;
		}

	uint8_t* addr = (uint8_t*)::mmap(0, bytecount, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0);

	if (addr == (uint8_t*)-1)
		{
		LOG_ERROR << "Failed to allocate a shared-memory segment of size " << bytecount / 1024 / 1024.0 << " MB: " 
			<< strerror(errno);

		shm_unlink(segmentName.c_str());

		return nullptr;
		}

	mSharedMemoryRecords[addr] = SharedMemoryRecord(addr, segmentName, shm_fd, bytecount);

	mTotalBytesShared += bytecount;

	fullMemoryBarrier();

	return addr;
	}

void OnDemandMemoryRegions::freeSharedRegion(uint8_t* addr)
	{
	boost::mutex::scoped_lock lock(mMutex);

	auto it = mSharedMemoryRecords.find(addr);
	lassert(it != mSharedMemoryRecords.end());

	if (::munmap(it->second.address(), it->second.size()))
		{
		LOG_CRITICAL << "munmap failed: " << strerror(errno);
		lassert(false);
		}

	mSharedMemoryRecords.erase(it);
	}

uint8_t* OnDemandMemoryRegions::allocateMappableRegion(uint64_t bytecount)
	{
	boost::mutex::scoped_lock lock(mMutex);

	uint8_t* addr = (uint8_t*)::mmap(0, bytecount, PROT_NONE, MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);

	if (addr == (uint8_t*)-1)
		{
		LOG_ERROR << "Failed to mmap " << bytecount / 1024 / 1024.0 << " MB of mappable memory address.";
		return nullptr;
		}

	mMappableMemoryRecords[addr] = MappableMemoryRecord(addr, bytecount);

	mTotalBytesMappable += bytecount;
	
	fullMemoryBarrier();

	return addr;
	}

//release a mappable region and unmap all mapped shared memory 
void OnDemandMemoryRegions::releaseMappableRegion(uint8_t* region)
	{
	boost::mutex::scoped_lock lock(mMutex);

	auto it = mMappableMemoryRecords.find(region);

	lassert(it != mMappableMemoryRecords.end());

	mTotalBytesMappable -= it->second.size();

	if (::munmap(it->second.address(), it->second.size()))
		{
		LOG_CRITICAL << "munmap failed: " << strerror(errno);
		lassert(false);
		}
	
	mMappableMemoryRecords.erase(it);
	}

//perform mappings
bool OnDemandMemoryRegions::mapShareableRegion(
        uint8_t* shareableRegion, 
        uint64_t shareableRegionOffset, 
        uint8_t* mappableRegion,
        uint64_t mappableRegionOffset,
        uint64_t bytesToMap
        )
	{
	boost::mutex::scoped_lock lock(mMutex);

	auto shareableIt = mSharedMemoryRecords.find(shareableRegion);

	if (shareableIt == mSharedMemoryRecords.end())
		return false;

	void* result = ::mmap(mappableRegion + mappableRegionOffset, 
			bytesToMap, 
			PROT_READ | PROT_WRITE, 
			MAP_FIXED | MAP_SHARED, 
			shareableIt->second.fileDescriptor(), 
			shareableRegionOffset
			);

	if (result == (void*)-1)
		{
		LOG_CRITICAL << "failed to map the shared region";
		return false;
		}

	lassert(result == mappableRegion + mappableRegionOffset);

	mMappedMemoryRecords[mappableRegion + mappableRegionOffset] = 
		MappedMemoryRecord(
			shareableRegion,
			shareableRegionOffset,
			mappableRegion,
			mappableRegionOffset,
			bytesToMap
			);

	uint8_t* activeRegionBase = mappableRegion + mappableRegionOffset;

	while (true)
		{
		auto it = mBlockingThreadConditions.lower_bound(activeRegionBase);
		if (it != mBlockingThreadConditions.end() && it->first >= activeRegionBase && it->first < activeRegionBase + bytesToMap)
			{
			for (auto ptr: it->second)
				ptr->notify_all();
			mBlockingThreadConditions.erase(it);
			}
		else
			return true;
		}
	}

bool OnDemandMemoryRegions::unmap(
        uint8_t* mappableRegion,
        uint64_t mappableRegionOffset,
        uint64_t bytesToMap
        )
	{
	boost::mutex::scoped_lock lock(mMutex);

	auto it = mMappedMemoryRecords.find(mappableRegion + mappableRegionOffset);
	
	lassert(it != mMappedMemoryRecords.end());
	lassert(bytesToMap == it->second.bytesToMap());

	mMappedMemoryRecords.erase(it);

	return ::munmap(mappableRegion + mappableRegionOffset, bytesToMap) == 0;
	}

uint64_t OnDemandMemoryRegions::bytesOfMappableRegions() const
	{
	return mTotalBytesMappable;
	}

uint32_t OnDemandMemoryRegions::blockedThreadCount() const
	{
	boost::mutex::scoped_lock lock(mMutex);

	return mBlockingThreads.size();
	}

void OnDemandMemoryRegions::getAllBlockingThreads(std::vector<BlockingThread>& outThreads)
	{
	boost::mutex::scoped_lock lock(mMutex);

	outThreads.clear();

	for (auto& kv: mBlockingThreads)
		outThreads.push_back(kv.second);
	}

